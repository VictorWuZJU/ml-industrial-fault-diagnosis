\section{Related Work}
\label{sec:related_work}

Research on industrial fault detection and diagnosis (FDD) spans several methodological lines, including classical model-based methods, signal-processing techniques, machine-learning approaches, and, more recently, deep-learning and edge-oriented architectures. Each line addresses different aspects of industrial systems—such as process physics, sensor behaviour, and computational constraints—and the recent literature continues to reflect this diversity.

\subsection{Classical model-based and signal-processing approaches}
Model-based diagnosis methods traditionally rely on observers, parity equations, Kalman filters and residual-generation schemes, where deviations between measured and estimated variables indicate abnormal states. These methods have been widely applied to mechanical drives, rotating machinery, process systems and power electronics, often with good interpretability and integration into existing control frameworks~\cite{goswami2021observer,du2019parity,patton2021trends,zhou2020robustcontrol}. However, their performance strongly depends on model accuracy, which degrades as equipment ages, physical parameters drift, or unmodelled disturbances arise. Extensions such as adaptive observers, sliding-mode observers and hybrid analytical/data-driven residual models attempt to mitigate some of these limitations~\cite{mohammadi2020adaptive,esfahani2021hybrid,pan2022multifault}, yet the need for calibrated physics-based models remains a practical constraint.

Signal-processing techniques represent another long-established line of work. Statistical time-domain indicators (e.g., RMS, kurtosis, crest factor), frequency-domain features (e.g., spectral peaks, fault-related harmonics), and time–frequency transforms (e.g., wavelets, WPT, EMD/VMD) continue to underpin many industrial diagnosis pipelines~\cite{lei2018signalprocessing,zimroz2020indicators,li2021demodulation,sharma2022gearbox}. Classical demodulation, envelope analysis and cepstrum-based approaches have been especially influential in bearing and gearbox diagnosis. These techniques remain attractive for their interpretability and low computational requirements, but they require careful feature selection, manual parameter tuning and domain expertise, and may lose sensitivity under variable speeds, strong noise, or compound faults~\cite{wang2020envelope,yang2022nonstationary,tang2021spectral}.

\subsection{Machine-learning approaches}
Machine-learning-based FDD methods reduce reliance on handcrafted thresholds by mapping extracted features to health states through classification or regression models. Common choices include support vector machines (SVM), random forests, gradient-boosting models, relevance vector machines and k-nearest-neighbour classifiers~\cite{wang2021svm,li2020randomforest,yao2022rvm,liu2020knn}. These models have been applied to bearings, induction motors, pumps and reciprocating compressors, often achieving improved robustness under moderate noise compared to traditional rule-based systems. Hybrid systems combining statistical features, multiscale decomposition and ML classifiers are also widely reported~\cite{he2021hybridml,zheng2022wpt,kim2021decomposition,zhou2021motor}.

Another active direction involves sparse-representation and dictionary-learning methods, which aim to extract discriminative fault signatures via learned basis atoms~\cite{peng2019sparse,pan2019dictionary,yu2023sparselm}. These methods can capture nonlinear and non-stationary characteristics that traditional spectral features might miss, though they may require computationally intensive optimisation.

\subsection{Deep learning}
Deep-learning-based FDD replaces manual feature extraction with learned hierarchical representations. Convolutional neural networks (CNNs) have been widely adopted for diagnosis using raw vibration signals, time–frequency maps, or multimodal fusion inputs~\cite{wen2020cnnraw,jiang2020multidomain,qi2021attention,gai2023multisensor}. Recurrent networks (LSTMs, GRUs) have been used to capture temporal correlations in machinery signals, particularly under variable operating conditions~\cite{zhao2020gru,wang2021temporal}. More recent models include 1-D and 2-D transformers, attention-guided feature extractors and graph neural networks for structured sensor arrays or spatial relationships~\cite{he2022transformer,yan2023gnn,chen2023spatiotemporal,shen2022hybrid}.

Several studies demonstrate that deep networks can outperform conventional ML pipelines in multi-fault and compound-fault settings and maintain performance under moderate noise or varying speeds~\cite{sun2021ensemble,hou2022compound,shi2023noiseaware}. Nonetheless, deep networks often require large datasets for training, may be sensitive to domain shift and can be computationally demanding, which complicates their deployment in embedded or edge environments.

\subsection{Real-time, embedded and edge-oriented diagnosis}
Recent work emphasizes diagnosis under real-time constraints, reflecting the migration of FDD from central servers to edge devices. Research on lightweight architectures explores pruning, quantisation, knowledge distillation, and compact network designs such as MobileNet variants, ShuffleNet, SqueezeNet, and micro-CNNs~\cite{park2021pruned,wu2022quantized,liu2021distilled,gao2023mobilenet}. These models aim to balance accuracy with memory and computational budgets for real-time industrial applications.

Edge-based diagnostic frameworks have been explored for pumps, gearboxes, induction motors and robotic systems, demonstrating the feasibility of deploying ML and DL models on industrial PCs, FPGAs, microcontrollers and single-board computers~\cite{chang2022embedded,wang2022edgecloud,sun2023fpga}. Research on streaming inference, online learning and incremental adaptation addresses the challenge of non-stationary signals and concept drift in field environments~\cite{gong2021incremental,zhang2021online,choi2023communication}.

Studies also investigate communication-efficient FDD architectures within IIoT and Industry 4.0 settings, where decisions must be made locally to reduce bandwidth usage or latency~\cite{deng2021distributed}. Although these approaches show promise, many works still report latency empirically without an explicit mathematical link to sampling frequency, sliding-window structure or inference-time guarantees, an issue that motivates the formulation introduced in this study.

Prior work spans a wide range of techniques—from model-based observers to deep networks and edge-optimized architectures. However, a consistent challenge across these lines is the absence of a unified mathematical framework that relates sampling, windowing, feature extraction, inference and decision-making under real-time constraints. The present study builds on the strengths of these existing approaches while addressing this gap through an explicit operator-based formulation and a latency-aware streaming algorithm.