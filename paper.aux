\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{vashishtha2025roadmap}
\citation{elbrawany2023survey}
\citation{zhao2025bearingreview}
\citation{wang2025portapplications}
\citation{montejano2025mlcomparison}
\citation{souza2023pipeline}
\citation{su2024phmreview}
\citation{lei2025datadriven}
\citation{hou2024lightweighttransformer}
\citation{gong2024resnet}
\citation{jiang2024vmdshufflenet}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:introduction}{{1}{1}{Introduction}{section.1}{}}
\citation{cacaco2025xai}
\citation{maged2024xaireview}
\citation{jang2023xaiindustrial}
\citation{vashishtha2025roadmap}
\citation{souza2023pipeline}
\citation{hou2024lightweighttransformer}
\citation{jiang2024vmdshufflenet}
\citation{montejano2025mlcomparison}
\citation{lei2025datadriven}
\citation{cacaco2025xai}
\citation{jang2023xaiindustrial}
\citation{vashishtha2025roadmap}
\citation{zhao2025bearingreview}
\citation{souza2023pipeline}
\citation{hou2024lightweighttransformer}
\citation{gong2024resnet}
\citation{jiang2024vmdshufflenet}
\citation{montejano2025mlcomparison}
\citation{lei2025datadriven}
\citation{cacaco2025xai}
\citation{jang2023xaiindustrial}
\citation{goswami2021observer}
\citation{du2019parity}
\citation{patton2021trends}
\citation{zhou2020robustcontrol}
\citation{mohammadi2020adaptive}
\citation{esfahani2021hybrid}
\citation{pan2022multifault}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}\protected@file@percent }
\newlabel{sec:related_work}{{2}{2}{Related Work}{section.2}{}}
\citation{lei2018signalprocessing}
\citation{zimroz2020indicators}
\citation{li2021demodulation}
\citation{sharma2022gearbox}
\citation{wang2020envelope}
\citation{yang2022nonstationary}
\citation{tang2021spectral}
\citation{wang2021svm}
\citation{li2020randomforest}
\citation{yao2022rvm}
\citation{liu2020knn}
\citation{he2021hybridml}
\citation{zheng2022wpt}
\citation{kim2021decomposition}
\citation{zhou2021motor}
\citation{peng2019sparse}
\citation{pan2019dictionary}
\citation{yu2023sparselm}
\citation{wen2020cnnraw}
\citation{jiang2020multidomain}
\citation{qi2021attention}
\citation{gai2023multisensor}
\citation{zhao2020gru}
\citation{wang2021temporal}
\citation{he2022transformer}
\citation{yan2023gnn}
\citation{chen2023spatiotemporal}
\citation{shen2022hybrid}
\citation{sun2021ensemble}
\citation{hou2022compound}
\citation{shi2023noiseaware}
\citation{park2021pruned}
\citation{wu2022quantized}
\citation{liu2021distilled}
\citation{gao2023mobilenet}
\citation{chang2022embedded}
\citation{wang2022edgecloud}
\citation{sun2023fpga}
\citation{gong2021incremental}
\citation{zhang2021online}
\citation{choi2023communication}
\citation{deng2021distributed}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Classical model-based and signal-processing approaches}{3}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Machine-learning approaches}{3}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Deep learning}{3}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Real-time, embedded and edge-oriented diagnosis}{3}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Problem Formulation and Mathematical Model}{4}{section.3}\protected@file@percent }
\newlabel{sec:problem_formulation}{{3}{4}{Problem Formulation and Mathematical Model}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Industrial Process and Sensing Model}{4}{subsection.3.1}\protected@file@percent }
\newlabel{eq:yk}{{1}{4}{Industrial Process and Sensing Model}{equation.3.1}{}}
\newlabel{eq:fk}{{2}{4}{Industrial Process and Sensing Model}{equation.3.2}{}}
\newlabel{eq:dataset}{{3}{4}{Industrial Process and Sensing Model}{equation.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Sliding-Window Data Representation}{4}{subsection.3.2}\protected@file@percent }
\newlabel{eq:Yk}{{4}{4}{Sliding-Window Data Representation}{equation.3.4}{}}
\newlabel{eq:kw}{{5}{4}{Sliding-Window Data Representation}{equation.3.5}{}}
\newlabel{eq:Yw}{{6}{4}{Sliding-Window Data Representation}{equation.3.6}{}}
\citation{lei2018signalprocessing}
\citation{wen2020cnnraw}
\citation{jiang2020multidomain}
\citation{example3}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Feature Extraction and Normalization}{5}{subsection.3.3}\protected@file@percent }
\newlabel{eq:phi}{{7}{5}{Feature Extraction and Normalization}{equation.3.7}{}}
\newlabel{eq:normalize}{{8}{5}{Feature Extraction and Normalization}{equation.3.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Classifier Model}{5}{subsection.3.4}\protected@file@percent }
\newlabel{eq:gtheta}{{9}{5}{Classifier Model}{equation.3.9}{}}
\newlabel{eq:pw}{{10}{5}{Classifier Model}{equation.3.10}{}}
\newlabel{eq:map}{{11}{5}{Classifier Model}{equation.3.11}{}}
\newlabel{eq:loss}{{12}{5}{Classifier Model}{equation.3.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Decision Logic and Performance Metrics}{6}{subsection.3.5}\protected@file@percent }
\newlabel{eq:alarm}{{13}{6}{Decision Logic and Performance Metrics}{equation.3.13}{}}
\newlabel{eq:uncertain}{{14}{6}{Decision Logic and Performance Metrics}{equation.3.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Real-Time Prediction Algorithm}{6}{section.4}\protected@file@percent }
\newlabel{sec:algorithm}{{4}{6}{Real-Time Prediction Algorithm}{section.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  Block diagram of the real-time diagnosis loop. Multi-sensor measurements $\mathbf  {y}_k$ are sampled at period $T_s$, buffered into sliding windows $\mathbf  {Y}_w$ of length $L$ with stride $S$, mapped to feature vectors $\mathbf  {z}_w$, normalized to $\tilde  {\mathbf  {z}}_w$, classified by the model $g_\theta (\cdot )$ into class probabilities $\mathbf  {p}_w$, and finally converted into diagnostic decisions $\hat  {f}_w$ and alarms via a decision logic module. }}{6}{figure.1}\protected@file@percent }
\newlabel{fig:loop}{{1}{6}{Block diagram of the real-time diagnosis loop. Multi-sensor measurements $\mathbf {y}_k$ are sampled at period $T_s$, buffered into sliding windows $\mathbf {Y}_w$ of length $L$ with stride $S$, mapped to feature vectors $\mathbf {z}_w$, normalized to $\tilde {\mathbf {z}}_w$, classified by the model $g_\theta (\cdot )$ into class probabilities $\mathbf {p}_w$, and finally converted into diagnostic decisions $\hat {f}_w$ and alarms via a decision logic module}{figure.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Example latency and complexity breakdown for different windowing and model configurations. $L$: window length, $S$: stride, $d$: feature dimension, $T_{\text  {feat}}$, $T_{\text  {inf}}$: measured WCETs on target edge device, $S T_s$: real-time budget, Margin: slack time.}}{7}{table.1}\protected@file@percent }
\newlabel{tab:latency}{{1}{7}{Example latency and complexity breakdown for different windowing and model configurations. $L$: window length, $S$: stride, $d$: feature dimension, $T_{\text {feat}}$, $T_{\text {inf}}$: measured WCETs on target edge device, $S T_s$: real-time budget, Margin: slack time}{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}System Architecture}{7}{subsection.4.1}\protected@file@percent }
\newlabel{eq:timing_constraint}{{15}{7}{System Architecture}{equation.4.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Streaming Window Algorithm}{7}{subsection.4.2}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Real-time ML-based fault diagnosis}}{8}{algorithm.1}\protected@file@percent }
\newlabel{alg:diagnosis}{{1}{8}{Pseudocode}{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Pseudocode}{8}{subsection.4.3}\protected@file@percent }
\newlabel{subsec:pseudocode}{{4.3}{8}{Pseudocode}{subsection.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Complexity and latency analysis}{8}{subsection.4.4}\protected@file@percent }
\newlabel{subsec:complexity}{{4.4}{8}{Complexity and latency analysis}{subsection.4.4}{}}
\newlabel{eq:complexity_scaling}{{16}{8}{Complexity and latency analysis}{equation.4.16}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Summary of datasets used in the experiments. Here $f_s$ is the sampling frequency, $|\mathcal  {F}|$ the number of health states, and $N_{\text  {tr}}$, $N_{\text  {te}}$ the number of training and test windows, respectively.}}{9}{table.2}\protected@file@percent }
\newlabel{tab:datasets}{{2}{9}{Summary of datasets used in the experiments. Here $f_s$ is the sampling frequency, $|\mathcal {F}|$ the number of health states, and $N_{\text {tr}}$, $N_{\text {te}}$ the number of training and test windows, respectively}{table.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experimental Setup}{9}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Data sources}{9}{subsection.5.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Windowing and data split parameters. The window length L and stride S are given in samples; the corresponding durations in milliseconds are obtained using the sampling frequency fs.}}{9}{table.3}\protected@file@percent }
\newlabel{tab:windowing_params}{{3}{9}{Windowing and data split parameters. The window length L and stride S are given in samples; the corresponding durations in milliseconds are obtained using the sampling frequency fs}{table.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Preprocessing and windowing}{9}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Model configurations and implementation details}{10}{subsection.5.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Model configurations used in the experimental study. Here d is the feature dimension, P the number of trainable parameters, and “Feat.” indicates whether a separate feature extractor $\phi $($\cdot $) is used (handcrafted or learned). Parameter counts are indicative and depend on the exact architecture and dataset.}}{10}{table.4}\protected@file@percent }
\newlabel{tab:model_configs}{{4}{10}{Model configurations used in the experimental study. Here d is the feature dimension, P the number of trainable parameters, and “Feat.” indicates whether a separate feature extractor $\phi $($\cdot $) is used (handcrafted or learned). Parameter counts are indicative and depend on the exact architecture and dataset}{table.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Evaluation metrics}{10}{subsection.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Results and Discussion}{11}{section.6}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Overall diagnostic performance aggregated across all datasets. Metrics are macro-averaged over fault classes. The proposed 1D-CNN (M6) attains the highest macro F1 while remaining compatible with the edge-device constraints.}}{11}{table.5}\protected@file@percent }
\newlabel{tab:overall_performance}{{5}{11}{Overall diagnostic performance aggregated across all datasets. Metrics are macro-averaged over fault classes. The proposed 1D-CNN (M6) attains the highest macro F1 while remaining compatible with the edge-device constraints}{table.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Diagnostic performance}{11}{subsection.6.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Macro F1 scores [\%] per dataset and model. The proposed 1D-CNN (M6) achieves the highest or statistically tied performance on all datasets, with the largest gain on the industrial pump data.}}{11}{table.6}\protected@file@percent }
\newlabel{tab:dataset_performance}{{6}{11}{Macro F1 scores [\%] per dataset and model. The proposed 1D-CNN (M6) achieves the highest or statistically tied performance on all datasets, with the largest gain on the industrial pump data}{table.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Confusion matrices for the proposed 1D-CNN (M6) on (a) CWRU and (b) industrial pump-line datasets. Rows correspond to true classes $f_w$, columns to predicted classes $\hat  {f}_w$. Most residual errors occur between neighbouring severity levels or between early fault and healthy-like conditions.}}{12}{figure.2}\protected@file@percent }
\newlabel{fig:confusion_matrix}{{2}{12}{Confusion matrices for the proposed 1D-CNN (M6) on (a) CWRU and (b) industrial pump-line datasets. Rows correspond to true classes $f_w$, columns to predicted classes $\hat {f}_w$. Most residual errors occur between neighbouring severity levels or between early fault and healthy-like conditions}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Real-time behaviour}{12}{subsection.6.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Latency and real-time margin on the target edge device for a representative configuration ($f_s = \SI {20}{\kilo \hertz }$, $L = 4096$, $S = 1024$). Times are given in milliseconds. The budget $S T_s$ is \SI {51.2}{\milli \second }.}}{12}{table.7}\protected@file@percent }
\newlabel{tab:latency}{{7}{12}{Latency and real-time margin on the target edge device for a representative configuration ($f_s = \SI {20}{\kilo \hertz }$, $L = 4096$, $S = 1024$). Times are given in milliseconds. The budget $S T_s$ is \SI {51.2}{\milli \second }}{table.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces ROC curves for healthy-vs-fault discrimination on the four datasets. Each curve corresponds to the proposed 1D-CNN (M6). High AUC values indicate strong separation between healthy and faulty conditions, with the pump-line data showing the most challenging case.}}{13}{figure.3}\protected@file@percent }
\newlabel{fig:roc}{{3}{13}{ROC curves for healthy-vs-fault discrimination on the four datasets. Each curve corresponds to the proposed 1D-CNN (M6). High AUC values indicate strong separation between healthy and faulty conditions, with the pump-line data showing the most challenging case}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Empirical CDF of per-window processing time $T_{\mathrm  {feat}} + T_{\mathrm  {inf}}$ on the edge device for selected models. The vertical dashed line indicates the budget $S T_s$. The proposed 1D-CNN (M6) maintains a comfortable margin with limited variability.}}{13}{figure.4}\protected@file@percent }
\newlabel{fig:cdf_latency}{{4}{13}{Empirical CDF of per-window processing time $T_{\mathrm {feat}} + T_{\mathrm {inf}}$ on the edge device for selected models. The vertical dashed line indicates the budget $S T_s$. The proposed 1D-CNN (M6) maintains a comfortable margin with limited variability}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Average CPU utilisation and memory footprint on the edge device during continuous streaming for the main model configurations. The proposed model balances diagnostic performance and resource usage.}}{14}{figure.5}\protected@file@percent }
\newlabel{fig:cpu_memory}{{5}{14}{Average CPU utilisation and memory footprint on the edge device during continuous streaming for the main model configurations. The proposed model balances diagnostic performance and resource usage}{figure.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Robustness and generalisation}{14}{subsection.6.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Macro F1 versus SNR for selected models on the pump-line dataset. The proposed 1D-CNN (M6) degrades more gracefully than feature-based baselines and maintains higher performance in the low-SNR regime.}}{15}{figure.6}\protected@file@percent }
\newlabel{fig:snr_f1}{{6}{15}{Macro F1 versus SNR for selected models on the pump-line dataset. The proposed 1D-CNN (M6) degrades more gracefully than feature-based baselines and maintains higher performance in the low-SNR regime}{figure.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Cross-domain macro F1 [\%] for selected train--test pairs. Rows indicate the model and columns the train--test domain pairs. The proposed 1D-CNN (M6) consistently outperforms the shallow CNN (M5) and the ELM baseline (M4) under domain shift.}}{15}{table.8}\protected@file@percent }
\newlabel{tab:cross_domain}{{8}{15}{Cross-domain macro F1 [\%] for selected train--test pairs. Rows indicate the model and columns the train--test domain pairs. The proposed 1D-CNN (M6) consistently outperforms the shallow CNN (M5) and the ELM baseline (M4) under domain shift}{table.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Discussion}{16}{subsection.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.4.1}Limitations}{16}{subsubsection.6.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.4.2}Threats to validity}{16}{subsubsection.6.4.2}\protected@file@percent }
\bibstyle{IEEEtran}
\bibdata{references}
\bibcite{vashishtha2025roadmap}{1}
\bibcite{elbrawany2023survey}{2}
\bibcite{zhao2025bearingreview}{3}
\bibcite{wang2025portapplications}{4}
\bibcite{montejano2025mlcomparison}{5}
\bibcite{souza2023pipeline}{6}
\bibcite{su2024phmreview}{7}
\bibcite{lei2025datadriven}{8}
\bibcite{hou2024lightweighttransformer}{9}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion and Future Work}{17}{section.7}\protected@file@percent }
\bibcite{gong2024resnet}{10}
\bibcite{jiang2024vmdshufflenet}{11}
\bibcite{cacaco2025xai}{12}
\bibcite{maged2024xaireview}{13}
\bibcite{jang2023xaiindustrial}{14}
\bibcite{goswami2021observer}{15}
\bibcite{du2019parity}{16}
\bibcite{patton2021trends}{17}
\bibcite{zhou2020robustcontrol}{18}
\bibcite{mohammadi2020adaptive}{19}
\bibcite{esfahani2021hybrid}{20}
\bibcite{pan2022multifault}{21}
\bibcite{lei2018signalprocessing}{22}
\bibcite{zimroz2020indicators}{23}
\bibcite{li2021demodulation}{24}
\bibcite{sharma2022gearbox}{25}
\bibcite{wang2020envelope}{26}
\bibcite{yang2022nonstationary}{27}
\bibcite{tang2021spectral}{28}
\bibcite{wang2021svm}{29}
\bibcite{li2020randomforest}{30}
\bibcite{yao2022rvm}{31}
\bibcite{liu2020knn}{32}
\bibcite{he2021hybridml}{33}
\bibcite{zheng2022wpt}{34}
\bibcite{kim2021decomposition}{35}
\bibcite{zhou2021motor}{36}
\bibcite{peng2019sparse}{37}
\bibcite{pan2019dictionary}{38}
\bibcite{yu2023sparselm}{39}
\bibcite{wen2020cnnraw}{40}
\bibcite{jiang2020multidomain}{41}
\bibcite{qi2021attention}{42}
\bibcite{gai2023multisensor}{43}
\bibcite{zhao2020gru}{44}
\bibcite{wang2021temporal}{45}
\bibcite{he2022transformer}{46}
\bibcite{yan2023gnn}{47}
\bibcite{chen2023spatiotemporal}{48}
\bibcite{shen2022hybrid}{49}
\bibcite{sun2021ensemble}{50}
\bibcite{hou2022compound}{51}
\bibcite{shi2023noiseaware}{52}
\bibcite{park2021pruned}{53}
\bibcite{wu2022quantized}{54}
\bibcite{liu2021distilled}{55}
\bibcite{gao2023mobilenet}{56}
\bibcite{chang2022embedded}{57}
\bibcite{wang2022edgecloud}{58}
\bibcite{sun2023fpga}{59}
\bibcite{gong2021incremental}{60}
\bibcite{zhang2021online}{61}
\bibcite{choi2023communication}{62}
\bibcite{deng2021distributed}{63}
\bibcite{pan2023distributed}{64}
\citation{*}
\gdef \@abspage@last{20}
